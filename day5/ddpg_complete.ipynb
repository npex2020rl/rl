{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import MSELoss\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from buffer import ReplayBuffer\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Define Q-network & policy-network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# critic network definition\n",
    "# multi-layer perceptron (with 2 hidden layers)\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden1, hidden2):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(obs_dim + act_dim, hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.fc3 = nn.Linear(hidden2, 1)\n",
    "        \n",
    "    \n",
    "    def forward(self, obs, act):\n",
    "        x = torch.cat([obs, act], dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        return self.fc3(x)\n",
    "    \n",
    "    \n",
    "# actor network definition\n",
    "# multi-layer perceptron (with 2 hidden layers)\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, ctrl_range, hidden1, hidden2):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(obs_dim, hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.fc3 = nn.Linear(hidden2, act_dim)\n",
    "        self.ctrl_range = ctrl_range\n",
    "        \n",
    "    def forward(self, obs):\n",
    "        x = F.relu(self.fc1(obs))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        return self.ctrl_range * torch.tanh(self.fc3(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Define DDPG agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPGAgent:\n",
    "    def __init__(self, obs_dim, act_dim, ctrl_range, hidden1, hidden2):\n",
    "        \n",
    "        # networks\n",
    "        self.actor = Actor(obs_dim, act_dim, ctrl_range, hidden1, hidden2)\n",
    "        self.critic = Critic(obs_dim, act_dim, hidden1, hidden2)\n",
    "        \n",
    "        # target networks\n",
    "        self.targ_actor = copy.deepcopy(self.actor)\n",
    "        self.targ_critic = copy.deepcopy(self.critic)\n",
    "        \n",
    "        \n",
    "    def act(self, obs):\n",
    "        # numpy ndarray to torch tensor\n",
    "        # we first add an extra dimension\n",
    "        obs = obs[np.newaxis]\n",
    "        with torch.no_grad():\n",
    "            obs_tensor = torch.Tensor(obs)\n",
    "            act_tensor = self.actor(obs_tensor)\n",
    "\n",
    "        # torch tensor to numpy ndarray\n",
    "        # remove extra dimension\n",
    "        action = act_tensor.numpy()\n",
    "        action = np.squeeze(action, axis=0)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.89769304 -1.2820984 ]\n"
     ]
    }
   ],
   "source": [
    "agent = DDPGAgent(4, 2, 3, 32, 32)\n",
    "action = agent.act(np.array([3., -1., 2., -5.]))\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Implement one-step param update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(agent, replay_buf, gamma, actor_optim, critic_optim, tau, batch_size):\n",
    "    # agent : agent with networks to be trained\n",
    "    # replay_buf : replay buf from which we sample a batch\n",
    "    # actor_optim / critic_optim : torch optimizers\n",
    "    # tau : parameter for soft target update\n",
    "    \n",
    "    batch = replay_buf.sample_batch(batch_size=batch_size)\n",
    "\n",
    "    # target construction does not need backward ftns\n",
    "    with torch.no_grad():\n",
    "        # unroll batch\n",
    "        obs = torch.Tensor(batch.obs)\n",
    "        act = torch.Tensor(batch.act)\n",
    "        next_obs = torch.Tensor(batch.next_obs)\n",
    "        rew = torch.Tensor(batch.rew)\n",
    "        done = torch.Tensor(batch.done)\n",
    "        \n",
    "        ################\n",
    "        # train critic #\n",
    "        ################\n",
    "        mask = torch.Tensor([1.]) - done\n",
    "        target = rew + gamma * mask * agent.targ_critic(next_obs, agent.targ_actor(next_obs))\n",
    "    \n",
    "    out = agent.critic(obs, act)\n",
    "    \n",
    "    loss_ftn = MSELoss()\n",
    "    critic_loss = loss_ftn(out, target)\n",
    "    # alternative : loss = torch.mean((target - out)**2)\n",
    "    \n",
    "    critic_optim.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    critic_optim.step()\n",
    "    \n",
    "    ###############\n",
    "    # train actor #\n",
    "    ###############\n",
    "    \n",
    "    # freeze critic during actor training (why?)\n",
    "    for p in agent.critic.parameters():\n",
    "        p.requires_grad_(False)\n",
    "    \n",
    "    actor_loss = -torch.mean(agent.critic(obs, agent.actor(obs)))\n",
    "    \n",
    "    actor_optim.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    actor_optim.step()\n",
    "    \n",
    "    \n",
    "    # unfreeze critic after actor training\n",
    "    for p in agent.critic.parameters():\n",
    "        p.requires_grad_(True)\n",
    "        \n",
    "    # soft target update (both actor & critic network)\n",
    "    for p, targ_p in zip(agent.actor.parameters(), agent.targ_actor.parameters()):\n",
    "        targ_p.data.copy_((1. - tau) * targ_p + tau * p)\n",
    "    for p, targ_p in zip(agent.critic.parameters(), agent.targ_critic.parameters()):\n",
    "        targ_p.data.copy_((1. - tau) * targ_p + tau * p)\n",
    "        \n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(agent, env, num_episodes=5):\n",
    "    \n",
    "    test_env = copy.deepcopy(env)\n",
    "    sum_scores = 0.\n",
    "    \n",
    "    for i in range(num_episodes):\n",
    "        obs = test_env.reset()\n",
    "        done = False\n",
    "        score = 0.\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.act(obs)\n",
    "            obs, rew, done, _ = test_env.step(action)\n",
    "            score += rew\n",
    "        sum_scores += score\n",
    "    avg_score = sum_scores / num_episodes\n",
    "    \n",
    "    return avg_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Combining these, we finally have..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, env, gamma, \n",
    "          actor_lr, critic_lr, tau, noise_std,\n",
    "          ep_len, num_updates, batch_size,\n",
    "          init_buffer=5000, buffer_size=100000,\n",
    "          start_train=2000, train_interval=50,\n",
    "          eval_interval=2000):\n",
    "    \n",
    "    actor_optim = Adam(agent.actor.parameters(), lr=actor_lr)\n",
    "    critic_optim = Adam(agent.critic.parameters(), lr=critic_lr)\n",
    "    \n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    act_dim = env.action_space.shape[0]\n",
    "    ctrl_range = env.action_space.high[0]\n",
    "    \n",
    "    replay_buf = ReplayBuffer(obs_dim, act_dim, buffer_size)\n",
    "    \n",
    "    \n",
    "    # main loop\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    step_count = 0\n",
    "    \n",
    "    for t in range(num_updates + 1):\n",
    "        if t < init_buffer:\n",
    "            # perform random action until we collect sufficiently many samples\n",
    "            # this is for exploration purpose\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            # executes noisy action\n",
    "            # a_t = \\pi(s_t) + N(0, \\sigma^2)\n",
    "            action = agent.act(obs) + noise_std * np.random.randn(act_dim)\n",
    "            action = np.clip(action, -ctrl_range, ctrl_range)\n",
    "            \n",
    "        next_obs, rew, done, _ = env.step(action)\n",
    "        step_count += 1\n",
    "        if step_count == ep_len:\n",
    "            # if the next_state is not terminal but done is set to True by gym env wrapper\n",
    "            done = False\n",
    "            \n",
    "        replay_buf.append(obs, action, next_obs, rew, done)\n",
    "        obs = next_obs\n",
    "        \n",
    "        if done == True or step_count == ep_len:\n",
    "            # reset environment if current environment reaches a terminal state \n",
    "            # or step count reaches predefined length\n",
    "            obs = env.reset()\n",
    "            done = False\n",
    "            step_count = 0\n",
    "        \n",
    "        \n",
    "        if t > start_train and t % train_interval == 0:\n",
    "            # start training after fixed number of steps\n",
    "            # this may mitigate overfitting of networks to the \n",
    "            # small number of samples collected during the initial stage of training\n",
    "            for _ in range(train_interval):\n",
    "                update(agent, replay_buf, gamma, actor_optim, critic_optim, tau, batch_size)\n",
    "\n",
    "        if t % eval_interval == 0:\n",
    "            score = evaluate(agent, env)\n",
    "            print('[iteration {}] evaluation score : {}'.format(t, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Let's test the code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space dim : 24 / action space dim : 4\n",
      "ctrl range :  1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sju5379/anaconda3/envs/npex/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('BipedalWalker-v3')\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "ctrl_range = env.action_space.high[0]\n",
    "\n",
    "print('observation space dim : {} / action space dim : {}'.format(obs_dim, act_dim))\n",
    "print('ctrl range : ', ctrl_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DDPGAgent(obs_dim=obs_dim, act_dim=act_dim, ctrl_range=ctrl_range, hidden1=256, hidden2=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "actor_lr = 1e-4\n",
    "critic_lr = 1e-3\n",
    "tau = 1e-3\n",
    "noise_std = 0.1\n",
    "ep_len = 500\n",
    "num_updates = 1000000\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sju5379/anaconda3/envs/npex/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iteration 0] evaluation score : -105.18096577831925\n",
      "[iteration 5000] evaluation score : -149.42335325775898\n",
      "[iteration 10000] evaluation score : -117.20837319950499\n",
      "[iteration 15000] evaluation score : -103.57316732793561\n",
      "[iteration 20000] evaluation score : -111.85199683076887\n",
      "[iteration 25000] evaluation score : -105.68771600659984\n",
      "[iteration 30000] evaluation score : -126.66331381985417\n",
      "[iteration 35000] evaluation score : -122.11559117571228\n",
      "[iteration 40000] evaluation score : -119.58603488314411\n",
      "[iteration 45000] evaluation score : -118.27418510137\n",
      "[iteration 50000] evaluation score : -122.26617781308087\n",
      "[iteration 55000] evaluation score : -128.72361778657395\n",
      "[iteration 60000] evaluation score : -128.63449127786168\n",
      "[iteration 65000] evaluation score : -138.25478470315934\n",
      "[iteration 70000] evaluation score : -139.09374152110217\n",
      "[iteration 75000] evaluation score : -133.98284320065378\n",
      "[iteration 80000] evaluation score : -138.70358006132116\n",
      "[iteration 85000] evaluation score : -142.0952392617763\n",
      "[iteration 90000] evaluation score : -155.53727279685896\n",
      "[iteration 95000] evaluation score : -139.3764902220936\n",
      "[iteration 100000] evaluation score : -132.15014817783896\n",
      "[iteration 105000] evaluation score : -144.79387624895259\n",
      "[iteration 110000] evaluation score : -141.63622872051386\n",
      "[iteration 115000] evaluation score : -92.8370997723686\n",
      "[iteration 120000] evaluation score : -136.66542108277395\n",
      "[iteration 125000] evaluation score : -111.23645445157169\n",
      "[iteration 130000] evaluation score : -103.16243046476313\n",
      "[iteration 135000] evaluation score : -134.72333072330372\n",
      "[iteration 140000] evaluation score : -130.53987086160788\n",
      "[iteration 145000] evaluation score : -126.94801707852153\n",
      "[iteration 150000] evaluation score : -128.64280081894051\n",
      "[iteration 155000] evaluation score : -111.53948140282323\n",
      "[iteration 160000] evaluation score : -145.28657385258703\n",
      "[iteration 165000] evaluation score : -141.799570406273\n",
      "[iteration 170000] evaluation score : -126.18522506447661\n",
      "[iteration 175000] evaluation score : -147.92116808763058\n",
      "[iteration 180000] evaluation score : -130.84239837187633\n",
      "[iteration 185000] evaluation score : -157.13046061598644\n",
      "[iteration 190000] evaluation score : -141.84345842323188\n",
      "[iteration 195000] evaluation score : -160.93182637810327\n",
      "[iteration 200000] evaluation score : -143.76986108937064\n",
      "[iteration 205000] evaluation score : -86.67981499425733\n",
      "[iteration 210000] evaluation score : -137.27488468670066\n",
      "[iteration 215000] evaluation score : -141.46050541384005\n",
      "[iteration 220000] evaluation score : -142.92186979475778\n",
      "[iteration 225000] evaluation score : -111.95031549010953\n",
      "[iteration 230000] evaluation score : -145.88625481648762\n",
      "[iteration 235000] evaluation score : -162.19309854999526\n",
      "[iteration 240000] evaluation score : -132.5422340982596\n",
      "[iteration 245000] evaluation score : -145.94171624002917\n",
      "[iteration 250000] evaluation score : -140.76004430616945\n",
      "[iteration 255000] evaluation score : -129.11189736471738\n",
      "[iteration 260000] evaluation score : -148.63964181196894\n",
      "[iteration 265000] evaluation score : -133.38942285504245\n",
      "[iteration 270000] evaluation score : -148.84684794784994\n",
      "[iteration 275000] evaluation score : -147.23814570948974\n",
      "[iteration 280000] evaluation score : -109.99351589006945\n",
      "[iteration 285000] evaluation score : -136.34519176638574\n",
      "[iteration 290000] evaluation score : -122.7571238429809\n",
      "[iteration 295000] evaluation score : -138.2261086362844\n",
      "[iteration 300000] evaluation score : -96.54375582732919\n",
      "[iteration 305000] evaluation score : -136.86287065820505\n",
      "[iteration 310000] evaluation score : -152.8415957431369\n",
      "[iteration 315000] evaluation score : -94.2662571788763\n",
      "[iteration 320000] evaluation score : -165.95996841851013\n",
      "[iteration 325000] evaluation score : -150.32150326543797\n",
      "[iteration 330000] evaluation score : -139.10127006716502\n",
      "[iteration 335000] evaluation score : -127.20757184962068\n",
      "[iteration 340000] evaluation score : -132.00810362179757\n",
      "[iteration 345000] evaluation score : -104.0899189719383\n",
      "[iteration 350000] evaluation score : -142.1262441783478\n",
      "[iteration 355000] evaluation score : -168.1758561925698\n",
      "[iteration 360000] evaluation score : -142.93875636914248\n",
      "[iteration 365000] evaluation score : -138.12432785514923\n",
      "[iteration 370000] evaluation score : -185.90278670895015\n",
      "[iteration 375000] evaluation score : -150.6319706544412\n",
      "[iteration 380000] evaluation score : -117.8066445359766\n",
      "[iteration 385000] evaluation score : -123.6262287641212\n",
      "[iteration 390000] evaluation score : -169.3375063151325\n",
      "[iteration 395000] evaluation score : -153.05606082780537\n",
      "[iteration 400000] evaluation score : -142.69592704246753\n",
      "[iteration 405000] evaluation score : -134.01059646921232\n",
      "[iteration 410000] evaluation score : -137.3849640467759\n",
      "[iteration 415000] evaluation score : -119.21587015289386\n",
      "[iteration 420000] evaluation score : -123.98896387378629\n",
      "[iteration 425000] evaluation score : -123.6522348858462\n",
      "[iteration 430000] evaluation score : -133.1066659177016\n",
      "[iteration 435000] evaluation score : -134.43975294683128\n",
      "[iteration 440000] evaluation score : -141.46203780538286\n",
      "[iteration 445000] evaluation score : -161.94741291748403\n",
      "[iteration 450000] evaluation score : -125.73999638392263\n",
      "[iteration 455000] evaluation score : -131.78732066308388\n",
      "[iteration 460000] evaluation score : -132.0443607135715\n",
      "[iteration 465000] evaluation score : -150.74107229197358\n",
      "[iteration 470000] evaluation score : -118.1237589907312\n",
      "[iteration 475000] evaluation score : -151.20740902809615\n",
      "[iteration 480000] evaluation score : -110.29956734816922\n",
      "[iteration 485000] evaluation score : -126.4944254231938\n",
      "[iteration 490000] evaluation score : -134.36539694093474\n",
      "[iteration 495000] evaluation score : -119.3660561039865\n",
      "[iteration 500000] evaluation score : -133.83564582955188\n",
      "[iteration 505000] evaluation score : -122.9830534831888\n",
      "[iteration 510000] evaluation score : -171.55534503704098\n",
      "[iteration 515000] evaluation score : -59.481579741257555\n",
      "[iteration 520000] evaluation score : -95.31011268088685\n",
      "[iteration 525000] evaluation score : -94.06207506835919\n",
      "[iteration 530000] evaluation score : -143.29584785047965\n",
      "[iteration 535000] evaluation score : -135.84994820992995\n",
      "[iteration 540000] evaluation score : -122.54683600215745\n",
      "[iteration 545000] evaluation score : -119.84938232630643\n",
      "[iteration 550000] evaluation score : -129.1149310784384\n",
      "[iteration 555000] evaluation score : -135.83206174021686\n",
      "[iteration 560000] evaluation score : -144.79150620670043\n",
      "[iteration 565000] evaluation score : -87.52588853742941\n",
      "[iteration 570000] evaluation score : -126.6913424846905\n",
      "[iteration 575000] evaluation score : -49.179620734154405\n",
      "[iteration 580000] evaluation score : -132.07200215310675\n",
      "[iteration 585000] evaluation score : -159.03517852979425\n",
      "[iteration 590000] evaluation score : -140.56159952636148\n",
      "[iteration 595000] evaluation score : -119.12622296000397\n",
      "[iteration 600000] evaluation score : -149.64245931958754\n",
      "[iteration 605000] evaluation score : -143.961131841729\n",
      "[iteration 610000] evaluation score : -176.55066955131974\n",
      "[iteration 615000] evaluation score : -117.98700452891573\n",
      "[iteration 620000] evaluation score : -169.60368568910135\n",
      "[iteration 625000] evaluation score : -155.19097305922648\n",
      "[iteration 630000] evaluation score : -164.9559536672755\n",
      "[iteration 635000] evaluation score : -159.45440153578932\n",
      "[iteration 640000] evaluation score : -168.23883329607972\n",
      "[iteration 645000] evaluation score : -150.21690298155653\n",
      "[iteration 650000] evaluation score : -138.65774643566039\n",
      "[iteration 655000] evaluation score : -169.16314479680804\n",
      "[iteration 660000] evaluation score : -150.92731456456278\n",
      "[iteration 665000] evaluation score : -140.81066483041792\n",
      "[iteration 670000] evaluation score : -171.30625635590604\n",
      "[iteration 675000] evaluation score : -114.90085991468875\n",
      "[iteration 680000] evaluation score : -146.60664862627647\n",
      "[iteration 685000] evaluation score : -160.42347763022946\n",
      "[iteration 690000] evaluation score : -104.07371254600139\n",
      "[iteration 695000] evaluation score : -147.41698566364838\n",
      "[iteration 700000] evaluation score : -150.33237674061834\n",
      "[iteration 705000] evaluation score : -138.06463632356582\n",
      "[iteration 710000] evaluation score : -143.74283829931318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iteration 715000] evaluation score : -152.84649391752228\n",
      "[iteration 720000] evaluation score : -144.79366149822053\n",
      "[iteration 725000] evaluation score : -135.76441646748708\n",
      "[iteration 730000] evaluation score : -135.9217606600875\n",
      "[iteration 735000] evaluation score : -119.40010283725296\n",
      "[iteration 740000] evaluation score : -133.21465693016407\n",
      "[iteration 745000] evaluation score : -199.10399893962625\n",
      "[iteration 750000] evaluation score : -116.17272406831023\n",
      "[iteration 755000] evaluation score : -130.77469738684812\n",
      "[iteration 760000] evaluation score : -138.87501428273188\n",
      "[iteration 765000] evaluation score : -139.0995160091315\n",
      "[iteration 770000] evaluation score : -111.87070219704533\n",
      "[iteration 775000] evaluation score : -69.03112494620791\n",
      "[iteration 780000] evaluation score : -149.1098507742875\n",
      "[iteration 785000] evaluation score : -139.85497107735662\n",
      "[iteration 790000] evaluation score : -164.39760835992072\n",
      "[iteration 795000] evaluation score : -162.20011362472016\n",
      "[iteration 800000] evaluation score : -101.51012575170004\n",
      "[iteration 805000] evaluation score : -143.1284993150504\n",
      "[iteration 810000] evaluation score : -152.85858228203853\n",
      "[iteration 815000] evaluation score : -146.12503846862722\n",
      "[iteration 820000] evaluation score : -151.7495974813455\n",
      "[iteration 825000] evaluation score : -145.186425177864\n",
      "[iteration 830000] evaluation score : -138.73369436933416\n",
      "[iteration 835000] evaluation score : -145.11683085658626\n",
      "[iteration 840000] evaluation score : -153.84481298135296\n",
      "[iteration 845000] evaluation score : -183.81134813881354\n",
      "[iteration 850000] evaluation score : -119.09825242687123\n",
      "[iteration 855000] evaluation score : -144.47909778685758\n",
      "[iteration 860000] evaluation score : -125.03020022445621\n",
      "[iteration 865000] evaluation score : -166.06437804869478\n",
      "[iteration 870000] evaluation score : -159.18560159326\n",
      "[iteration 875000] evaluation score : -125.9695358889908\n",
      "[iteration 880000] evaluation score : -98.46853541751294\n",
      "[iteration 885000] evaluation score : -163.27284045311336\n",
      "[iteration 890000] evaluation score : -127.51834980496433\n",
      "[iteration 895000] evaluation score : -137.90232369201652\n",
      "[iteration 900000] evaluation score : -156.68920827354233\n",
      "[iteration 905000] evaluation score : -146.72723413209874\n",
      "[iteration 910000] evaluation score : -115.58631208126471\n",
      "[iteration 915000] evaluation score : -184.78927848266758\n",
      "[iteration 920000] evaluation score : -144.55809498849442\n",
      "[iteration 925000] evaluation score : -139.54865978007768\n",
      "[iteration 930000] evaluation score : -157.81200853996242\n",
      "[iteration 935000] evaluation score : -150.047194120935\n",
      "[iteration 940000] evaluation score : -149.72938566548535\n",
      "[iteration 945000] evaluation score : -148.24927109470312\n",
      "[iteration 950000] evaluation score : -192.5186603127773\n",
      "[iteration 955000] evaluation score : -154.6623385623578\n",
      "[iteration 960000] evaluation score : -147.54197908420937\n",
      "[iteration 965000] evaluation score : -141.33742208560238\n",
      "[iteration 970000] evaluation score : -148.82109193833944\n",
      "[iteration 975000] evaluation score : -146.38142913326396\n",
      "[iteration 980000] evaluation score : -149.30421794094653\n",
      "[iteration 985000] evaluation score : -161.64568869405372\n",
      "[iteration 990000] evaluation score : -152.92090127931255\n",
      "[iteration 995000] evaluation score : -105.34501324988378\n",
      "[iteration 1000000] evaluation score : -80.24177934586314\n"
     ]
    }
   ],
   "source": [
    "train(agent, env, gamma,\n",
    "      actor_lr, critic_lr, tau, noise_std,\n",
    "      ep_len, num_updates, batch_size,\n",
    "      init_buffer=5000, buffer_size=1000000,\n",
    "      start_train=2000, train_interval=50,\n",
    "      eval_interval=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Watch the trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score :  -71.75615367728673\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "done = False\n",
    "score = 0.\n",
    "\n",
    "while not done:\n",
    "    env.render()\n",
    "    obs, rew, done, _ = env.step(agent.act(obs))\n",
    "    score += rew\n",
    "    \n",
    "env.close()\n",
    "print('score : ', score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

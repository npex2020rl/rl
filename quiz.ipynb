{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz\n",
    "\n",
    "In quiz, we review <span style = 'color:blue'>DDPG </span> which is simple yet powerful off-policy actor-critic algorithm. As we already have some experience in writing DPPG-related codes, you may refer to the code we wrote in day5 if you have difficulty filling the blank. Please feel free to e-mail me if you have some problems, either with running the code or solving quiz. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import MSELoss\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from buffer import ReplayBuffer\n",
    "import gym\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Define Q-network & policy-network\n",
    "\n",
    "In this section, we define an actor and a critic as 2-layer NNs. Note that you may choose different non-linear activation instead of ReLu and test the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# critic network definition\n",
    "# multi-layer perceptron (with 2 hidden layers)\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden1, hidden2):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(obs_dim + act_dim, hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.fc3 = nn.Linear(hidden2, 1)\n",
    "        \n",
    "    \n",
    "    def forward(self, obs, act):\n",
    "        x = torch.cat([obs, act], dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        return self.fc3(x)\n",
    "    \n",
    "    \n",
    "# actor network definition\n",
    "# multi-layer perceptron (with 2 hidden layers)\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, ctrl_range, hidden1, hidden2):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(obs_dim, hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.fc3 = nn.Linear(hidden2, act_dim)\n",
    "        self.ctrl_range = ctrl_range\n",
    "        \n",
    "    def forward(self, obs):\n",
    "        x = F.relu(self.fc1(obs))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        return self.ctrl_range * torch.tanh(self.fc3(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define DDPG agent\n",
    "\n",
    "What should our agent have? In princple, all we need is a single actor network(and critic network in some cases), which perform an appropriate action for given observation. For simplicity, we assume that the agent also has target networks as its components(althouth we don't need them after training is done). <span style='color:red'> You have some codes to fill in here. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPGAgent:\n",
    "    def __init__(self, obs_dim, act_dim, ctrl_range, hidden1, hidden2):\n",
    "        \n",
    "        # networks\n",
    "        self.actor = Actor(obs_dim, act_dim, ctrl_range, hidden1, hidden2)\n",
    "        self.critic = Critic(obs_dim, act_dim, hidden1, hidden2)\n",
    "        \n",
    "        # target networks\n",
    "        self.targ_actor = copy.deepcopy(self.actor)\n",
    "        self.targ_critic = copy.deepcopy(self.critic)\n",
    "        print('networks loaded')\n",
    "        \n",
    "    def act(self, obs):\n",
    "        \n",
    "        obs = obs[np.newaxis]  # we first add an extra dimension\n",
    "        with torch.no_grad():\n",
    "            obs_tensor = torch.Tensor(obs)  # numpy ndarray to torch tensor\n",
    "            \n",
    "            ### TODO (Question 1) ### You need to implement how to compute action when given an observation\n",
    "            \n",
    "\n",
    "        action = act_tensor.numpy()  # torch tensor to numpy ndarray\n",
    "        action = np.squeeze(action, axis=0)  # remove extra dimension\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agent = DDPGAgent(4, 2, 3, 32, 32)\n",
    "obs = np.array([3., -1., 2., -5.])\n",
    "print('observation type = {} / observation shape = {}'.format(type(obs), obs.shape))\n",
    "action = agent.act(obs)\n",
    "print('selected action : ', action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Implement one-step param update\n",
    "\n",
    "In this section, we will learn how to perform one-step parameter updates of the actor and the critic. <span style='color:red'> Again, we have some work left for you. </span>\n",
    "\n",
    "## 3.1. Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(agent, replay_buf, gamma, actor_optim, critic_optim, tau, batch_size):\n",
    "    # agent : agent with networks to be trained\n",
    "    # replay_buf : replay buf from which we sample a batch\n",
    "    # actor_optim / critic_optim : torch optimizers\n",
    "    # tau : parameter for soft target update\n",
    "    \n",
    "    batch = replay_buf.sample_batch(batch_size=batch_size)\n",
    "\n",
    "    # target construction does not need backward ftns\n",
    "    with torch.no_grad():\n",
    "        # unroll batch\n",
    "        obs = torch.Tensor(batch.obs)\n",
    "        act = torch.Tensor(batch.act)\n",
    "        next_obs = torch.Tensor(batch.next_obs)\n",
    "        rew = torch.Tensor(batch.rew)\n",
    "        done = torch.Tensor(batch.done)\n",
    "        \n",
    "        ################\n",
    "        # train critic #\n",
    "        ################\n",
    "        mask = torch.Tensor([1.]) - done\n",
    "        \n",
    "        ### TODO (Question 2) ### You should construct a target tensor which is used to train the critic\n",
    "        \n",
    "    \n",
    "    out = agent.critic(obs, act)\n",
    "    \n",
    "    loss_ftn = MSELoss()\n",
    "    critic_loss = loss_ftn(out, target)\n",
    "    # alternative : loss = torch.mean((target - out)**2)\n",
    "    \n",
    "    critic_optim.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    critic_optim.step()\n",
    "    \n",
    "    ###############\n",
    "    # train actor #\n",
    "    ###############\n",
    "    \n",
    "    # freeze critic during actor training (why?)\n",
    "    for p in agent.critic.parameters():\n",
    "        p.requires_grad_(False)\n",
    "    \n",
    "    actor_loss = -torch.mean(agent.critic(obs, agent.actor(obs)))\n",
    "    \n",
    "    \n",
    "    ### TODO (Question 3) ### After loss construction, you need to compute gradient \n",
    "    # by calling the appropriate optimizer.\n",
    "    # Please implement it.\n",
    "    \n",
    "    \n",
    "    \n",
    "    # unfreeze critic after actor training\n",
    "    for p in agent.critic.parameters():\n",
    "        p.requires_grad_(True)\n",
    "        \n",
    "    # soft target update (both actor & critic network)\n",
    "    for p, targ_p in zip(agent.actor.parameters(), agent.targ_actor.parameters()):\n",
    "        targ_p.data.copy_((1. - tau) * targ_p + tau * p)\n",
    "    for p, targ_p in zip(agent.critic.parameters(), agent.targ_critic.parameters()):\n",
    "        targ_p.data.copy_((1. - tau) * targ_p + tau * p)\n",
    "        \n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Evaluation\n",
    "\n",
    "when training, you are going to check whether the agent is being trained or not, by periodically evaluating your agent on the environment. This is how we do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(agent, env, num_episodes=5):\n",
    "    \n",
    "    assert num_episodes > 0\n",
    "    \n",
    "    test_env = copy.deepcopy(env)\n",
    "    sum_scores = 0.\n",
    "    \n",
    "    for i in range(num_episodes):\n",
    "        \n",
    "        obs = test_env.reset()\n",
    "        done = False\n",
    "        score = 0.\n",
    "        \n",
    "        while not done:\n",
    "            if i == 0:\n",
    "                test_env.render()\n",
    "            action = agent.act(obs)\n",
    "            obs, rew, done, _ = test_env.step(action)\n",
    "            score += rew\n",
    "        sum_scores += score\n",
    "        if i == 0:\n",
    "            test_env.close()\n",
    "\n",
    "    avg_score = sum_scores / num_episodes\n",
    "    \n",
    "\n",
    "    return avg_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Test\n",
    "\n",
    "Just check that the function you have defined correctly works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = 'BipedalWalkerHardcore-v3'\n",
    "env = gym.make(env_id)\n",
    "obs_dim, act_dim = env.observation_space.shape[0], env.action_space.shape[0]\n",
    "ctrl_range = env.action_space.high[0]\n",
    "\n",
    "print('env id : ', env_id)\n",
    "print('observation space dim = {} / action space dim = {}'.format(obs_dim, act_dim))\n",
    "print('action space range = {}'.format(ctrl_range))\n",
    "random_agent = DDPGAgent(obs_dim, act_dim, ctrl_range, 32, 32)\n",
    "\n",
    "print('score : ', evaluate(random_agent, env, num_episodes=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Writer definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Writer:\n",
    "    def __init__(self, path, num_log):\n",
    "        self.scores = np.zeros((num_log, 2))  # 1st col : time step, 2nd col : score\n",
    "        self.head = 0\n",
    "        self.end = num_log\n",
    "        self.path = os.path.join(path, 'scores.npy')  # path of the location to store the scores\n",
    "        \n",
    "    def write(self, step, score):\n",
    "        assert self.head < self.end\n",
    "        \n",
    "        self.scores[self.head] = np.array([step, score])\n",
    "        self.head += 1\n",
    "        \n",
    "    def save(self):\n",
    "        np.save(self.path, self.scores)\n",
    "        self.scores = np.zeros((self.end, 2))\n",
    "        self.head = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Combining these, we finally have...\n",
    "\n",
    "Now we have all components we need for complete training! Let's see how training the agent looks like. <span style = 'color:red'> You have some blanks to fill in. This will be the last part you have to code yourself. </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, env, gamma, \n",
    "          actor_lr, critic_lr, tau, noise_std,\n",
    "          ep_len, num_updates, batch_size,\n",
    "          init_buffer=5000, buffer_size=100000,\n",
    "          start_train=2000, train_interval=50,\n",
    "          eval_interval=2000, writer=None):\n",
    "    \n",
    "    actor_optim = Adam(agent.actor.parameters(), lr=actor_lr)\n",
    "    critic_optim = Adam(agent.critic.parameters(), lr=critic_lr)\n",
    "    \n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    act_dim = env.action_space.shape[0]\n",
    "    ctrl_range = env.action_space.high[0]\n",
    "    \n",
    "    replay_buf = ReplayBuffer(obs_dim, act_dim, buffer_size)\n",
    "    \n",
    "    \n",
    "    # main loop\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    step_count = 0\n",
    "    \n",
    "    for t in range(num_updates + 1):\n",
    "        if t < init_buffer:\n",
    "            # perform random action until we collect sufficiently many samples\n",
    "            # this is for exploration purpose\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            # executes noisy action\n",
    "            # a_t = \\pi(s_t) + N(0, \\sigma^2)\n",
    "            \n",
    "            ### TODO (Question 4) ### Implement a_t = \\pi(s_t) + N(0, \\sigma^2)\n",
    "            # Hint : One can generate a random sample from N(0, \\sigma^2) using noise_std * np.random.randn(act_dim)\n",
    "            # You may look at our previous code we worked on in day5\n",
    "            \n",
    "            \n",
    "            action = np.clip(action, -ctrl_range, ctrl_range)\n",
    "            \n",
    "        next_obs, rew, done, _ = env.step(action)\n",
    "        step_count += 1\n",
    "        if step_count == ep_len:\n",
    "            # if the next_state is not terminal but done is set to True by gym env wrapper\n",
    "            done = False\n",
    "            \n",
    "        replay_buf.append(obs, action, next_obs, rew, done)\n",
    "        obs = next_obs\n",
    "        \n",
    "        if done == True or step_count == ep_len:\n",
    "            # reset environment if current environment reaches a terminal state \n",
    "            # or step count reaches predefined length\n",
    "            obs = env.reset()\n",
    "            done = False\n",
    "            step_count = 0\n",
    "        \n",
    "        \n",
    "        if t > start_train and t % train_interval == 0:\n",
    "            # start training after fixed number of steps\n",
    "            # this may mitigate overfitting of networks to the \n",
    "            # small number of samples collected during the initial stage of training\n",
    "            for _ in range(train_interval):\n",
    "                update(agent, replay_buf, gamma, actor_optim, critic_optim, tau, batch_size)\n",
    "\n",
    "        if t % eval_interval == 0:\n",
    "            score = evaluate(agent, env)\n",
    "            print('[iteration {}] evaluation score : {}'.format(t, score))\n",
    "            if writer is not None:\n",
    "                writer.write(t, score)\n",
    "                \n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Let's test the code!\n",
    "\n",
    "Congratulation! You've just finished your Quiz. The rest of the code will show you how training is done in practice and how to analyze the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = 'LunarLanderContinuous-v2'\n",
    "env = gym.make(env_id)\n",
    "obs_dim, act_dim = env.observation_space.shape[0], env.action_space.shape[0]\n",
    "ctrl_range = env.action_space.high[0]\n",
    "\n",
    "print('env id : ', env_id)\n",
    "print('observation space dim = {} / action space dim = {}'.format(obs_dim, act_dim))\n",
    "print('action space range = {}'.format(ctrl_range))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DDPGAgent(obs_dim=obs_dim, act_dim=act_dim, ctrl_range=ctrl_range, hidden1=256, hidden2=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Hyperparameters setup\n",
    "\n",
    "We introduce some well-tuned hyperparameters for DDPG. You may use them without any modification, or just try other choices for fun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "actor_lr = 1e-4\n",
    "critic_lr = 1e-3\n",
    "tau = 1e-3\n",
    "noise_std = 0.1\n",
    "ep_len = 500\n",
    "eval_interval = 5000\n",
    "num_updates = 200000\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pth = os.getcwd()\n",
    "print('evaluation history will be saved at : ', pth)\n",
    "writer = Writer(pth, num_updates // eval_interval + 1)\n",
    "print(writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Let's train!\n",
    "\n",
    "Don't worry about the performance; even if your agent does not perform well on the task, it will have little effect on your quiz score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(agent, env, gamma,\n",
    "      actor_lr, critic_lr, tau, noise_std,\n",
    "      ep_len, num_updates, batch_size,\n",
    "      init_buffer=5000, buffer_size=1000000,\n",
    "      start_train=2000, train_interval=50,\n",
    "      eval_interval=5000, writer=writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Watch the trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "done = False\n",
    "score = 0.\n",
    "\n",
    "while not done:\n",
    "    env.render()\n",
    "    obs, rew, done, _ = env.step(agent.act(obs))\n",
    "    score += rew\n",
    "    \n",
    "env.close()\n",
    "print('score : ', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.load(pth + '/scores.npy')\n",
    "\n",
    "steps = scores[:, 0] / 1000.\n",
    "score_arr = scores[:, 1]\n",
    "\n",
    "plt.plot(steps, score_arr)\n",
    "\n",
    "plt.title(env_id)\n",
    "plt.xlabel(r'step ($\\times 10^3 $)')\n",
    "plt.ylabel('score')\n",
    "plt.xlim(steps[0], steps[-1])\n",
    "\n",
    "plt.grid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
